{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@Author: Haihui Pan\n",
    "@Date: 2021-11-4\n",
    "@Ref: https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGrad自动微分\n",
    "\n",
    "* 反向传播(back propagation,BP)是训练神经网络最常见的算法。在BP中，模型的参数会根据损失函数计算出来的梯度来进行更新。\n",
    "\n",
    "* 为了计算梯度，PyTorch的***torch.autograd***提供了自动求导机制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "\n",
    "#单层神经网络\n",
    "x=torch.ones(5) #input\n",
    "y=torch.zeros(3)\n",
    "w=torch.randn(5,3,requires_grad=True)\n",
    "b=torch.randn(3,requires_grad=True)\n",
    "z=torch.matmul(x,w)+b\n",
    "\n",
    "loss=binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：你可以在创建Tensor时就设置requires_grad，也可以在创建之后通过调用x.requires_grad_(True)来进行设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 我们用来构造计算图的张量函数实际上是***Function***的对象。这个对象知道如何进行前馈计算，以及在反向传播过程中如何计算梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x0000029084EFE408>\n",
      "<BinaryCrossEntropyWithLogitsBackward object at 0x0000029084EFEB48>\n"
     ]
    }
   ],
   "source": [
    "#通过tensor.grad可以获取相关的反向传播函数\n",
    "print(z.grad_fn)\n",
    "print(loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算梯度\n",
    "\n",
    "* 为了优化神经网络的参数，我们需要计算Loss对于参数的梯度。为了计算梯度，我们可以调用***loss.backward()***,可以通过tensor.grad来获取对应的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3173, 0.0152, 0.2986],\n",
      "        [0.3173, 0.0152, 0.2986],\n",
      "        [0.3173, 0.0152, 0.2986],\n",
      "        [0.3173, 0.0152, 0.2986],\n",
      "        [0.3173, 0.0152, 0.2986]])\n",
      "tensor([0.3173, 0.0152, 0.2986])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 禁用自动梯度计算\n",
    "\n",
    "* 默认情况先，将Tensor的requires_grad=True会导致Pytorch保留计算历史和支持梯度计算。例如，在测试场景下我们不需要去计算模型的梯度，我们只需要获取模型前馈计算的结果。通过torch.no_grad()模块，可以使得避免保存Tensor的计算历史。\n",
    "\n",
    "* 在迁移学习中，我们可以需要去冻结某些参数，这时候需要将对应参数的requires_grad设置为false。\n",
    "* torch.no_grad()可以加速前馈计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z=torch.matmul(x,w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "#不用保存计算历史\n",
    "with torch.no_grad():\n",
    "    z=torch.matmul(x,w)+b\n",
    "    print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#通过设置detach()，也可以达到相同效果\n",
    "z=torch.matmul(x,w)+b\n",
    "z_det=z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"autograd_01.png\",width=\"40%\",height=\"40%\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"autograd_01.png\",width=\"40%\",height=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从概念上讲，在一个由函数组成的向无环图(DAG)中，autograd会保存Tensor和所有的操作。在DAG中，叶子是输入Tensor,根是输出Tensor。通过从根到叶跟踪这个图，可以使用链式法则自动计算梯度。\n",
    "\n",
    "* 在前馈计算过程中，autograd同时做两件事:\n",
    "  * 根据定义的操作计算对应的结果Tensor\n",
    "  * 在DAG中维护操作的gradient function\n",
    "  \n",
    "\n",
    "* 当最终的输出Tensor（如计算得到的loss）调用.backward()时开始，autograd之后开始计算:\n",
    "  * 计算每一个.grad_fn的梯度\n",
    "  * 在.grad属性中累加Tensor的梯度\n",
    "  * 利用链式法则，一直反向计算到叶子节点\n",
    "\n",
    "注：PyTorch中的DAG是动态的，计算图是从头开始重新创建的。在每次调用.backward()之后，autograd开始填充新的图形。这允许你在模型中使用控制流语句，如果需要您可以在每次迭代中更改形状、大小和操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 在一些场景中，我们的loss输出为标量。但是，在一些场景下，我们的loss输出为一个向量,或者是其他张量。在这种情况中，PyTorch允许你去计算***Jacobian product***（并不是真正的梯度）。\n",
    "\n",
    "* 给定函数$y=f(x),y=(y_{1},...,y_{m}),x=(x_{1},...,x_{n})$,则使用Jacobian矩阵来表示$y$对$x$的梯度：\n",
    "\n",
    "$J=\\bigl(\\begin{smallmatrix}\n",
    " \\frac{\\partial y_{1}}{\\partial x_{1}}& \\cdots &  \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\ \n",
    "\\vdots  & \\ddots  & \\vdots \\\\ \n",
    " \\frac{\\partial y_{m}}{\\partial x_{1}}& \\cdots  & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "\\end{smallmatrix}\\bigr)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 在PyTorch中并不是计算Jacobian矩阵，而是计算$v^{T}J$，其中$v^{T}$是给定的input。只要在backward中传入参数$v$即可，其中$v$预原始的Tensor应该一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n",
      "Second call\n",
      " tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.],\n",
      "        [4., 4., 4., 4., 8.]])\n",
      "Third call\n",
      " tensor([[12.,  6.,  6.,  6.,  6.],\n",
      "        [ 6., 12.,  6.,  6.,  6.],\n",
      "        [ 6.,  6., 12.,  6.,  6.],\n",
      "        [ 6.,  6.,  6., 12.,  6.],\n",
      "        [ 6.,  6.,  6.,  6., 12.]])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.eye(5, requires_grad=True)\n",
    "out=(inp+1).pow(2)\n",
    "\n",
    "#计算梯度，当输入不是一个标量时需要传入一个与inp相同shape的单位向量，否则计算v^T*J时会出错\n",
    "out.backward(torch.ones_like(inp),retain_graph=True)\n",
    "#计算Jacobian porduct\n",
    "print(\"First call\\n\", inp.grad)\n",
    "\n",
    "#如果不清零梯度，则会进行梯度的累加\n",
    "out.backward(torch.ones_like(inp),retain_graph=True)\n",
    "print(\"Second call\\n\", inp.grad)\n",
    "\n",
    "#如果不清零梯度，则会进行梯度的累加\n",
    "out.backward(torch.ones_like(inp),retain_graph=True)\n",
    "print(\"Third call\\n\", inp.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：当第二次使用相同的参数去调用backward时，对于inp的梯度结果是不一样的。这是由于在进行反向传播时，PyTorch会自动累加梯度，即将计算出来的梯度加上当前Tensor.grad上。如果你只需要去计算当前的梯度，则需要通过将Tensor的梯度设置为0。在实际应用中，optimizer.zero_grad()可以直接将所有的Tensor.grad清零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After zeroing grad\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n"
     ]
    }
   ],
   "source": [
    "#清零梯度，则会将先前累加的梯度设置为0\n",
    "inp.grad.zero_()\n",
    "\n",
    "out.backward(torch.ones_like(inp),retain_graph=True)\n",
    "print(\"After zeroing grad\\n\",inp.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
